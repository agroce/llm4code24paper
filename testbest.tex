%%
%% This is file `sample-sigconf-biblatex.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf-biblatex')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-biblatex.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf,natbib=false]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2024}
\acmYear{2024}
\acmDOI{XXXXXXX.XXXXXXX}



%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%


%%
%% The majority of ACM publications use numbered citations and
%% references, obtained by selecting the acmnumeric BibLaTeX style.
%% The acmauthoryear BibLaTeX style switches to the "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the acmauthoryear style of
%% citations and references.
%%
%% Bibliography style

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Software Testing is the Ideal Software Engineering Application
  for Present-Generation Large Language Models}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
% \author{Ben Trovato}
% \authornote{Both authors contributed equally to this research.}
% \email{trovato@corporation.com}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \streetaddress{P.O. Box 1212}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
%   \postcode{43017-6221}
% }

% \author{Lars Th{\o}rv{\"a}ld}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%   \city{Hekla}
%   \country{Iceland}}
% \email{larst@affiliation.org}

% \author{Valerie B\'eranger}
% \affiliation{%
%   \institution{Inria Paris-Rocquencourt}
%   \city{Rocquencourt}
%   \country{France}
% }

% \author{Aparna Patel}
% \affiliation{%
%  \institution{Rajiv Gandhi University}
%  \streetaddress{Rono-Hills}
%  \city{Doimukh}
%  \state{Arunachal Pradesh}
%  \country{India}}

% \author{Huifen Chan}
% \affiliation{%
%   \institution{Tsinghua University}
%   \streetaddress{30 Shuangqing Rd}
%   \city{Haidian Qu}
%   \state{Beijing Shi}
%   \country{China}}

% \author{Charles Palmer}
% \affiliation{%
%   \institution{Palmer Research Laboratories}
%   \streetaddress{8600 Datapoint Drive}
%   \city{San Antonio}
%   \state{Texas}
%   \country{USA}
%   \postcode{78229}}
% \email{cpalmer@prl.com}

% \author{John Smith}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%   \city{Hekla}
%   \country{Iceland}}
% \email{jsmith@affiliation.org}

% \author{Julius P. Kumquat}
% \affiliation{%
%   \institution{The Kumquat Consortium}
%   \city{New York}
%   \country{USA}}
% \email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Without denying the utility of current-generation Large Language
Models (LLMs) for assisting developers in writing code, we propose
that at present the most promising application of LLMs to software
engineering can be found in the problem of software testing.  The
consequences of generating faulty or insecure code are limited or
nearly eliminated when the code in question is test code, rather than
production' code.  Given the frequency with which current LLMs
generate incorrect code or answers to questions about code, this
reduction of the costs of failure (and thus the costs of verification of
LLM output) makes using LLMs in testing more attractive than in actual
development.  Furthermore, many software testing tasks that are not
code-generation tasks benefit from ``good-enough'' answers, without
requiring extreme accuracy (or precision) and allow an LLM to replace
human effort that is often so cost-ineffective that in practice it is
not attempted.  This may make powerful testing technique, such as
mutation testing, that are currently limited in efficacy by costs,
more practical; in a positive feedback loop, better test evaluation
tools further reduce the costs of ``bad'' LLM-generated test code, by
making it easier to detect.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10010940.10010992.10010998.10011001</concept_id>
<concept_desc>Software and its engineering~Dynamic analysis</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011074.10011099.10011102.10011103</concept_id>
<concept_desc>Software and its engineering~Software testing and debugging</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Dynamic analysis}
\ccsdesc[500]{Software and its engineering~Software testing and debugging}

\keywords{large language models, code generation, automated testing,
  test evaluation}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

Much of the excitement (and fear) associated with the use of Large
Language Models (LLMs) in software engineering has been associated
with the idea of LLMs as assistants to, or replacements for,
developers writing production code.  This paper argues that, given the
present state of LLMs, they are better suited to tasks related to
\emph{testing} code than to the core software engineering task of
producing the code that is to be used in the real world.  Current
generation LLMs are powerful, but are also prone to (unpredictable)
error and in need of close human supervision.  Concerns about the
correctness of LLM-generated code are greatly reduced in the context
of using LLMs to generate tests; testing tasks imply that production
code already exists.  The existence of this code gives LLMs a rich
context that likely allows them to perform better, and (even more
importantly) provides a simple check on the ``reasonableness'' of the
generated tests.  If LLM-produced tests fail for existing, correct
code, or do not improve objective, automated metrics such as code
coverage or mutation score, they are, without costly human supervision
or effort, easily seen to be inadequate.  Uses of LLMs in testing
tasks go beyond the simple idea of generating unit tests, but further
applications may be best understood in the context of the core
differences between generating production code and generating test
code, which we discuss first.

\section{LLM-Based Code Generation: Production Code vs. Test Code}

The rationale behind our argument that software testing is the most
promising application of current-day LLMs, given their capabiities,
can be most easily understood by examining the differences between
using LLMs to generate \emph{production} code (application code that is
executed when the code is deployed:  from the standpoint of testing,
the ``code under test'' (CUT)) and using LLMs to
generate \emph{test code}.  We proceed by examining the possible outcomes of
both tasks.

\subsection{Case 1: Success}

If an LLM generates correct, efficient production or test code, there is in one sense no
difference: the result is as desired.  However, there is still an
advantage in the case of test code:  it may not be neccessary to have
a human inspect it and determine its validity.  Operational code, in
most important contexts, must be inspected by another human being
before used (even code generated by professional human programmers is
usually subjected to substantial inspection in serious development
efforts).  LLM-generated test cases can be reviewed, of course, as
some human test code is subjected to inspection.  However, test code
can also be used without inspection, as if it were the output of a
fuzzer or automated testing tool, which is only examined if the test
produced fails.

\subsection{Case 2: Obvious Failure}

An LLM may also generate code that is obviously bad, and either fails
to compile (e.g., type system violations) or fails under almost all
runtime conditions.  Such code is, often, correctable to code of case
1 or case 3, with little effort.  In the case of test code, runtime
failures take the form of failing tests that should not fail,
e.g. where the LLM has produced an over-strict or simply incorrect
test oracle.  In these cases, the code can often be corrected by
properly weakening the oracle or by modifying the expected output to
match the real (correct) output of code.  Obvious (in terms of
visibility) failures in non-test code may requires substantially more
debugging effort, but it is reasonable to consider this case one where
neither kind of code generation has an obvious advantage.

\subsection{Case 3: Subtle Failure}

The most dangerous case for LLM-generated code, as with
human-generated code, is that of subtle failure which only exhibits
under rare input conditions.  For production code the consequences can
be arbitrary; the faulty code may not be detected until it is
deployed, with the usual potentially disastrous consequences and
costs.

A particularly important subtle failure mode of LLM-generated
production code is
code that is ``correct'' but \emph{insecure}.  Previous studies have
shown that, like code taken from sources such as stackoverflow,
LLM-code may be prone to taking the ``easy path'' of functionality
without security~\cite{pearce2022asleep,sandoval2023lost,siddiq2023generate}.  Given the
difficulty of testing for insecurity and the cost or impossibility of
``building security in after the fact,'' this can be a particularly
disastrous failure mode, and one that is almost always subtle.  There
is, essentially, no equivalent for this failure mode in test code,
which may fail to check for security, but in general cannot itself
introduce a vulnerability.

In fact, the consequences of rare failure in test code are generally
bounded.  A test is fundamentally either too weak or too strong.  That
is, it either does not fail when it should, or fails when it should
not fail.  The latter case, if subtle and only exposed after some
change to the code under test, is subtle in the sense that it is not
immediately detected, but can only be subtle in the sense that it is
hard to understand as a test failure if it \emph{exposes a complexity
  in the specification of the software.}  In our view, test code that
exposes such a complexity, especially a subtle one, is not a
``failure'' at all, but is instead a \emph{contribution to a software
  development effort}.  No such statement can be made about a subtle
bug in a program.

The remaining case is test code that subtly fails to catch some
erroneous aspects of behavior of the program under test.  However,
this is, essentially, the typical case for all individual tests,
manually or automatically constructed.  Production code is generally
\emph{uniquely} responsible for certain behaviors of a program; test code is
generally only partially responsible for detecting flaws in behavior,
in well-designed test efforts.  It is seldom the case that anyone
assumes a particular test is meant to catch all possible faults in a
program, or even in the code it executes.  Therefore, test code that detects at
least some bad behaviors is always useful, and this minimal criteria
can be easily established by using simple, fully automated techniques
such as mutation testing.

It is well known that LLMs often vary in their performance of a task,
sometimes more and sometimes less successful; in testing, it may often
be practical to simply use a large number of LLM generated tests for
the same code, assuming that redundnacy will be low cost (after all,
in fuzzing thousands of ineffective tests are executed for each
``useful'' test).  An equivalent approach for production code seems
unlikely to usually be possible.


\section{Beyond Generating Unit Tests}

The discussion above largely focuses on using LLMs to generate ``test
code''---typically, unit tests.  However, LLMs are also well-suited
for more esoteric testing tasks, including some that do not involve
generating code at all.

\subsection{Other Test Generation Tasks}

The context of existing production code gives LLMs more to work with,
and the context of existing tests for production code may give LLMs
even more to work with: LLMs can be used to augment existing tests
with better oracles~\cite{OracleGEN}.  This is a particulary promising
application, since the tendency of oracle quality to lag behind code coverage
is an enduring and under-addressed problem in software testing~\cite{MindGap}.

One critical problem in software testing is that while developers are
at least familliar with writing unit tests, few have experience in
writing property-based tests or fuzz-drivers~\cite{goldstein2022some}, a different task
that is often perceived as very difficult.  Generalizing oracle
generation from specific to parameterized unit tests is a potential
long-term solution to this problem, and there is already work underway
to generate fuzz drivers using LLMs~\cite{zhang2023understanding},
including by Google's OSS-Fuzz team, one of the most important
industrial fuzzing efforts~\cite{ossfuzzllm}.

Finally, LLMs can directly be used as fuzzers, for compilers and other
language tools~\cite{xia2023universal} or even
deep-learning libraries themselves~\cite{LLMFuzz}.

\subsection{Non-Code-Generation Testing Tasks}

Mutation testing has long been one of the most promising ways to
evaluate testing efforts; however, it is also expensive, both in terms
of computational and human effort.  Recently, LLMs have been used to
significantly reduce the computational costs of mutation testing, by
predicting (without running them) which mutants will kill which
tests~\cite{ContextPMT}.  On obvious next step is to apply LLMs to the
human-effort-intensive problems of identifying equivalent
mutants~\cite{TCE} and prioritizing which mutants to examine~\cite{StatMut}.

\section{Conclusion}

We consider it fortunate that LLMs are likely highly suited to testing
tasks, because, frankly, software developers generally do not enjoy
testing~\cite{DevsHateTesting}, and therefore often do not put sufficient effort into
testing~\cite{DevsDontTestEnough} or reviewing test code~\cite{TestCodeReview}.  Removing the burden of unpleasant and onerous (and often
poorly done) tasks from humans is, we claim, the ideal use of
automation.  Better testing is, additionally, even more critical in a
world where some portion of the code under test is to be generated by
LLMs, which may produce new and surprising kinds of bugs that humans
have trouble imagining, and thus addressing with appropriate tests.

Finally, as a provocative point, we suggest that in the long term,
LLMs may offer a solution to one of the more long-standing and
controversial problems of software engineering~\cite{mackenzie2004mechanizing}:
DeMillo, Lipton, and Perlis famously argued in the late 1970s that
program verification could never be ``proof'' in the sense that
mathematical proof is, because program proofs are simply \emph{too boring} to
be subjected to the social aspects of mathematical
proof~\cite{de1979social}.  LLMs, however, may be applicable to the Leibnizian
dream of program proof, in that they may be capable of many of the
critical functions of human analysis of proofs, but without the limitation of being
bored by tedious detail and extreme length.


\bibliographystyle{plain}
\bibliography{bibliography}


\end{document}
\endinput
%%
%% End of file `sample-sigconf-biblatex.tex'.
