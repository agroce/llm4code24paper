%%
%% This is file `sample-sigconf-biblatex.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf-biblatex')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-biblatex.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf,natbib=false]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2024}
\acmYear{2024}
\acmDOI{XXXXXXX.XXXXXXX}

\usepackage{code}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%


%%
%% The majority of ACM publications use numbered citations and
%% references, obtained by selecting the acmnumeric BibLaTeX style.
%% The acmauthoryear BibLaTeX style switches to the "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the acmauthoryear style of
%% citations and references.
%%
%% Bibliography style

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Software Testing is the Ideal Software Engineering Application
  for Present-Generation Large Language Models}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
% \author{Ben Trovato}
% \authornote{Both authors contributed equally to this research.}
% \email{trovato@corporation.com}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \streetaddress{P.O. Box 1212}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
%   \postcode{43017-6221}
% }

% \author{Lars Th{\o}rv{\"a}ld}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%   \city{Hekla}
%   \country{Iceland}}
% \email{larst@affiliation.org}

% \author{Valerie B\'eranger}
% \affiliation{%
%   \institution{Inria Paris-Rocquencourt}
%   \city{Rocquencourt}
%   \country{France}
% }

% \author{Aparna Patel}
% \affiliation{%
%  \institution{Rajiv Gandhi University}
%  \streetaddress{Rono-Hills}
%  \city{Doimukh}
%  \state{Arunachal Pradesh}
%  \country{India}}

% \author{Huifen Chan}
% \affiliation{%
%   \institution{Tsinghua University}
%   \streetaddress{30 Shuangqing Rd}
%   \city{Haidian Qu}
%   \state{Beijing Shi}
%   \country{China}}

% \author{Charles Palmer}
% \affiliation{%
%   \institution{Palmer Research Laboratories}
%   \streetaddress{8600 Datapoint Drive}
%   \city{San Antonio}
%   \state{Texas}
%   \country{USA}
%   \postcode{78229}}
% \email{cpalmer@prl.com}

% \author{John Smith}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%   \city{Hekla}
%   \country{Iceland}}
% \email{jsmith@affiliation.org}

% \author{Julius P. Kumquat}
% \affiliation{%
%   \institution{The Kumquat Consortium}
%   \city{New York}
%   \country{USA}}
% \email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Without denying the utility of current-generation Large Language
Models (LLMs) for assisting developers in writing code, we propose
that at present the most promising application of LLMs to software
engineering can be found in the problem of software testing.  The
consequences of generating faulty or insecure code are limited or
nearly eliminated when the code in question is test code, rather than
production code.  Given the frequency with which current LLMs
generate incorrect code or answers to questions about code, this
reduction of the costs of failure (and thus the costs of verification of
LLM output) makes using LLMs in testing more attractive than in actual
development.  Furthermore, many software testing tasks that are not
code-generation tasks benefit from ``good-enough'' answers, without
requiring extreme accuracy (or precision) and allow an LLM to replace
human effort that is often so cost-ineffective that in practice it is
not attempted.  This may make powerful testing techniques, such as
mutation testing, that are currently limited in efficacy by costs,
more practical; in a positive feedback loop, better test evaluation
tools further reduce the costs of ``bad'' LLM-generated test code, by
making it easier to detect.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10010940.10010992.10010998.10011001</concept_id>
<concept_desc>Software and its engineering~Dynamic analysis</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011074.10011099.10011102.10011103</concept_id>
<concept_desc>Software and its engineering~Software testing and debugging</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Dynamic analysis}
\ccsdesc[500]{Software and its engineering~Software testing and debugging}

\keywords{large language models, code generation, automated testing,
  test evaluation}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

Much of the excitement (and fear) associated with the use of Large
Language Models (LLMs) in software engineering has been associated
with the idea of LLMs as assistants to, or replacements for,
developers writing production code.  This paper argues that, given the
present state of LLMs, they are better suited to tasks related to
\emph{testing} code than to the core software engineering task of
producing the code that is to be used in the real world.  Current
generation LLMs are powerful, but are also prone to (unpredictable)
error and in need of close human supervision.  Concerns about the
correctness of LLM-generated code are greatly reduced in the context
of using LLMs to generate tests; testing tasks imply that production
code already exists.  The existence of this code gives LLMs a rich
context that likely allows them to perform better, and (even more
importantly) provides a simple check on the ``reasonableness'' of the
generated tests.  If LLM-produced tests fail for existing, correct
code, or do not improve objective, automated metrics such as code
coverage or mutation score~\cite{demillo1978hints}, they are, without costly human supervision
or effort, easily seen to be inadequate.  Uses of LLMs in testing
tasks go beyond the simple idea of generating unit tests, but further
applications may be best understood in the context of the core
differences between generating production code and generating test
code, which we discuss first.

\section{LLM-Based Code Generation: Production Code vs. Test Code}



\begin{table*}[t]
  {\large
  \begin{tabular}{l|l|l}
    & {\bf How Hard is the Specification to Satisfy?} & {\bf
                                                        How Hard is
                                                        the
                                                        Specification
                                                        to Check?} \\
    \hline
    Production Code & Hard  (Program Correctness) & Hard (Full Program
                                                    Verification) \\
    & & (requires proof or at least very high quality test suite) \\
    \hline
    Test Code & Easy (Utility) & Easy (Improved Test Metrics) \\
    & & (e.g., coverage or mutants killed increase) \\
    \hline
  \end{tabular}
}
\caption{Fundamental difference between production and test code}
\label{tab:diff}
  \end{table*}


The rationale behind our argument that software testing is the most
promising application of current-day LLMs, given their capabiities,
can be most easily understood by examining the differences between
using LLMs to generate \emph{production} code (application code that is
executed when the code is deployed:  from the standpoint of testing,
the ``code under test'' (CUT)) and using LLMs to
generate \emph{test code}, code not executed by users or in
deployment, but used to find flaws in production code.  We proceed by examining the possible outcomes of
both tasks.

\subsection{Case 1: Success}

If an LLM generates correct, efficient production or test code, there is in one sense no
difference: the result is as desired.  However, there is still an
advantage in the case of test code:  it may not be neccessary to have
a human inspect it and determine its validity.  Production code, in
most real-world contexts, must be inspected by another human being
before used (even code generated by professional human programmers is
usually subjected to substantial inspection in serious development
efforts).  LLM-generated test cases \emph{can} be reviewed, of course,
just as some human test code is subjected to inspection.  However, test code
can also be used without inspection, as if it were the output of a
fuzzer or automated testing tool, which is only examined if the test
produced fails.

\subsection{Case 2: Obvious Failure}

An LLM may also generate code that is obviously bad, and either fails
to compile (e.g., type system violations) or fails under almost all
runtime conditions.  Such code is, often, correctable to code of case
1 or case 3, with little effort.  In the case of test code, runtime
failures take the form of failing tests that should not fail,
e.g. where the LLM has produced an over-strict or simply incorrect
test oracle.  In these cases, the code can often be corrected by
properly weakening the oracle or by modifying the expected output to
match the real (correct) output of code.  Even obvious (in terms of
visibility) failures in production code may sometimes require substantially more
debugging effort, to ensure fixing the simple bug does not introduce a
more subtle problem, but it is reasonable to consider this case one where
neither kind of code generation has an obvious advantage.  This leaves
us with the third, and most dangerous, outcome of code generation.

\subsection{Case 3: Subtle Failure}


The most troublesome case for LLM-generated code, as with
human-generated code, is that of subtle failure, faults which only exhibit
under rare input conditions.  For production code the consequences can
be arbitrary; the faulty code may not be detected until it is
deployed, with the usual potentially disastrous consequences and
costs.  The large research field of software testing essentially
exists in order to limit such occurrences.

A particularly important subtle failure mode of LLM-generated
production code is
code that is ``correct'' but \emph{insecure}.  Previous studies have
shown that, like code taken from sources such as stackoverflow~\cite{stackoverflow},
LLM-code may be prone to taking the ``easy path'' of functionality
without security~\cite{pearce2022asleep,sandoval2023lost,siddiq2023generate}.  Given the
difficulty of testing for insecurity and the cost or impossibility of
``building security in after the fact,'' this can be a particularly
disastrous failure mode, and one that is almost always subtle.  There
is, essentially, no equivalent for this failure mode in test code,
which may fail to check for security, but in general cannot itself
introduce a vulnerability.

In fact, the consequences of rare failure in test code are generally
bounded.  A test is fundamentally either too weak or too strong.  That
is, it either does not fail when it should, or fails when it should
not fail.  The latter case, if subtle and only exposed after some
change to the code under test, is subtle in the sense that it is not
immediately detected, but can only be subtle in the sense that it is
hard to understand as a bug in the test when it \emph{exposes a complexity
  in the specification of the software.}  In our view, test code that
exposes such a complexity, especially a subtle one, is not a
``failure'' at all, but is instead a \emph{contribution to a software
  development effort}.  No such positive statement can be made about a subtle
bug in a program!

The remaining case is test code that subtly fails to catch some
erroneous aspects of behavior of the program under test.  However,
this is, essentially, the typical case for all individual tests,
manually or automatically constructed.  Production code is generally
\emph{uniquely} responsible for certain behaviors of a program; test code is
generally only partially responsible for detecting flaws in behavior,
in well-designed test efforts.  It is seldom the case that anyone
assumes a particular test is meant to catch all possible faults in a
program, or even in the code it executes.  Therefore, test code that \emph{detects at
least some bad behaviors} is always useful, and this minimal criteria
can be easily established by using simple, fully automated techniques
such as mutation testing.

It is well known that LLMs often vary in their performance of a task,
sometimes more and sometimes less successful; in testing, it may often
be practical to simply use a large number of LLM generated tests for
the same code, assuming that redundnacy will be low cost (after all,
in fuzzing thousands of ineffective tests are executed for each
``useful'' test).  An equivalent approach for production code seems
unlikely to usually be possible.

One way to consider the fundamental difference is shown in
Table~\ref{tab:diff}.  Generating production code requires satisfying
a complex, hard-to-construct (even for humans), usually implicit but
also problem-specific, specification: that the
code has what is essentially a conceptually single desired behavior.  Determining
that a proposed solution fails to satisfy this difficult-to-satisfy
specification is also extremely hard: it is the general program verification
problem, if guarantees are desired, and the general program testing
problem, if the problem is rephrased as finding a counterexample
showing the LLM output is incorrect.

In contrast, generating a useful test means only generating code that
increases test utility: that is, code that can catch \emph{some} bugs.  No one expects
that a single test (or even test suite) will detect all possible
bugs.  Further, determining if a proposed incremental addition to
tests is useful is often easy: if the test increases either code
coverage or killed mutants, then the test is clearly of value.
Fuzzers frequently automatically determine ``interestingness'' of thousands of
potential tests per second.

Asking an LLM to solve an easier problem where failure or success is
easy to evaluate is obviously a more likely-to-succeed strategy in
cases where the LLM is not infallible.

\section{LLM-Specific Virtues of Test Code}

In addition to the fundamental differences in the tasks discussed above, some
unusual features of test code make it particularly suitable for LLM
generation.  

Researchers have defined a wide 
range of metrics, including BLEU~\cite{BLEU}, ROUGE-L~\cite{ROUGE}, and even more code-specific metrics that perform 
AST matching and dataflow analysis such as CodeBLEU~\cite{CodeBLEU},
CrystalBLEU~\cite{CrystalBLEU} and CodeBertScore~\cite{CodeBERTScore},
to determine how well LLM-generated code matches ``something a
developer might write.''  
Fundamentally, these lexical metrics are just an approximation of what
generated code should look like, structurally and syntactically; they
only accidentally capture any semantic value, due to their generality.  As noted above, for
test code similarly generic \emph{semantic} checks are available,
including most obviously code coverage and ability to kill mutants.
These are suitable for fully automated reinforcement learning based approaches,
without requiring the human intervention often invoked to check
production code via Reinforcement Learning with Human Feedback (RLHF)~\cite{rlhf}.


Software code has been fundamentally shown to be \emph{natural}: in other words there are common, repetitive patterns that can be 
learned by neural models~\cite{naturalnessofcode}. This naturalness is a big reason why techniques such as code generation work so well, with LLMs 
learning common patterns of code from the vast corpus of open source projects. 

Source code for tests is much more structurally regular than even
normal source code.  For example, unit tests are usually both loop and
condition free, and can be broken down into setup/call/check
patterns that are common across many tests.  This has been observed to
the point that formal models including only assignments, calls, and
assertions have been successfully used as paradigms for unit test
generation~\cite{tstlsttt}.  Further, the widespread
use of a small number of unit testing frameworks (e.g., JUnit,
GoogleTest, and Python's unittest module) makes test code even more
similar across programs.

Modern LLMs have been pretrained on millions of source code tokens, thus these models have a strong understanding of code semantics 
and behavior. This can be seem with CodeBERT~\cite{codebert} embeddings outperforming prior work such as Code2Vec~\cite{code2vec} 
and TF-IDF~\cite{tfidf}, with the CodeBERT vectors capturing some
notion of semantic similarity rather than pure token match (as in techniques 
such as TF-IDF).  These training bases contain a large number of
tests, with many signals including naming and structure distinguishing
the test code within these corpuses, making
models ``aware'' of the tightly constrained nature of test code.
Thus one can generate entire test suites with a powerful model such as ChatGPT~\cite{gpttestgen, siddiq2023empirical}. 
Even smaller models, trained with this relationship~\cite{catlm, starcoder} show significant promise, generating tests that compile, execute successfully, and 
increase coverage.


\section{Beyond Generating Unit Tests}

The discussion above largely focuses on using LLMs to generate ``test
code''---typically, unit tests.  However, LLMs are also well-suited
for more esoteric testing tasks, including some that do not involve
generating code at all.

\subsection{Other Test Generation Tasks}

The context of existing production code gives LLMs more to work with,
and the context of existing tests for production code may give LLMs
even more to work with: LLMs can be used to augment existing tests
with (better) oracles~\cite{OracleGEN}.  This is a particulary promising
application, since the tendency of oracle quality to lag behind code coverage
is an enduring and under-addressed problem in software testing~\cite{MindGap}.

Another critical problem in software testing is that while developers are
at least familliar with writing unit tests, few have experience in
writing property-based tests or fuzz-drivers~\cite{goldstein2022some}, a different task
that is often perceived as very difficult.  Generalizing oracle
generation from specific to parameterized unit tests is a potential
long-term solution to this problem, and there is already work underway
to generate fuzz drivers using LLMs~\cite{zhang2023understanding},
including by Google's OSS-Fuzz team, one of the most important
industrial fuzzing efforts~\cite{ossfuzzllm}.  In fact, even a free
general-purpose tool such as ChatGPT 3.5 can generate fuzz harnesses
for specific fuzzing tools, e.g., Trail of Bits' DeepState ``unit
fuzzing'' tool~\cite{goodman2018deepstate}, as shown in Figure ~\ref{fig:ds}.

\begin{figure}
  {\scriptsize
    \begin{code}
{\bf QUERY:}

Can you write a DeepState harness to test a binary search
routine called using the function binsearch:

int binsearch(int a[], size\_t N, int target)

where binsearch returns -1 if the target is not found,
otherwise the location of target in a?      

{\bf RESPONSE:}
      
\#include <deepstate/DeepState.hpp>

extern "C" \{
  int binsearch(int a[], size\_t N, int target);
\}

using namespace deepstate;

TEST(BinarySearch, BasicFunctionality) \{
  // Define the size of the array
  size\_t array\_size = 10;

  // Generate a random array
  int a[array\_size];
  for (size\_t i = 0; i < array\_size; ++i) {
    a[i] = DeepState\_Int(); // Generate a random integer
  }

  // Sort the array (binary search requires a sorted array)
  std::sort(a, a + array\_size);

  // Generate a random target value
  int target = DeepState\_Int();

  // Perform binary search
  int result = binsearch(a, array\_size, target);

  // Check the result
  if (result == -1) \{
    ASSERT\_FALSE(std::binary\_search(a, a + array\_size, target));
  \} else \{
    ASSERT\_EQ(a[result], target);
  \}
\}
\end{code}
}
\caption{ChatGPT 3.5 generated response to query to generate a DeepState harness to test C
  binary search function.}
\label{fig:ds}
  \end{figure}

Finally, LLMs can directly be used as fuzzers, for compilers and other
language tools~\cite{xia2023universal} or even
deep-learning libraries themselves~\cite{LLMFuzz}.

\subsection{Non-Code-Generation Testing Tasks}

Mutation testing has long been one of the most promising ways to
evaluate testing efforts; however, it is also expensive, both in terms
of computational and human effort.  Recently, LLMs have been used to
significantly reduce the computational costs of mutation testing, by
predicting (without running them) which mutants will kill which
tests~\cite{ContextPMT}.  An obvious next step is to apply LLMs to the
human-effort-intensive problems of identifying equivalent
mutants~\cite{TCE} and prioritizing which mutants to examine~\cite{StatMut}.

\section{Conclusion}

We consider it fortunate that LLMs are likely highly suited to testing
tasks, because, frankly, software developers generally do not enjoy
testing~\cite{DevsHateTesting}, and therefore often do not put sufficient effort into
testing~\cite{DevsDontTestEnough} or reviewing test code~\cite{TestCodeReview}.  Removing the burden of unpleasant and onerous (and often
poorly done) tasks from humans is, we claim, the ideal use of
automation.  Better testing is, additionally, even more critical in a
world where some portion of the code under test is to be generated by
LLMs, which may produce new and surprising kinds of bugs that humans
have trouble imagining, and thus addressing with appropriate tests.

Finally, as a provocative point, we suggest that in the long term,
LLMs may offer a solution to one of the more long-standing and
controversial problems of software engineering~\cite{mackenzie2004mechanizing}:
DeMillo, Lipton, and Perlis famously argued in the late 1970s that
program verification could never be ``proof'' in the sense that
mathematical proof is, because program proofs are simply \emph{too boring} to
be subjected to the social aspects of mathematical
proof~\cite{de1979social}.  LLMs, however, may be applicable to the Leibnizian
dream of program proof, in that they may be capable of many of the
critique and refinement functions of human analysis of proofs, but without the limitation of being
bored by tedious detail and extreme length.  LLMs are already being
used to help generate program proofs~\cite{Baldur}; in the long run they may help
make such proofs more meaningful and valuable, as well.


\bibliographystyle{plain}
\bibliography{bibliography}


\end{document}
\endinput
%%
%% End of file `sample-sigconf-biblatex.tex'.
